from typing import Dict, Tuple

from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    RegexTokenizer,
    StopWordsRemover,
    CountVectorizer,
    IDF,
)


def build_text_feature_pipeline(
    *,
    input_col: str = "text",
    output_col: str = "features",
    vocab_size: int = 20000,
    min_df: int = 5,
    binary_tf: bool = True,   # ‚úÖ mare boost pt vitezƒÉ
) -> Tuple[Pipeline, Dict]:
    """
    Pipeline NLP Spark optimizat pentru Big Data:
      text
        -> tokens (lowercase, regex simplu)
        -> filtered_tokens (stopwords EN)
        -> tf (CountVectorizer)
        -> idf
        -> features
    """

    # 1Ô∏è‚É£ Tokenizer ‚Äì regex simplu + lowercase
    tokenizer = RegexTokenizer(
        inputCol=input_col,
        outputCol="tokens",
        pattern=r"[^a-zA-Z]+",   # mult mai rapid dec√¢t \W+
        toLowercase=True,
        minTokenLength=2
    )

    # 2Ô∏è‚É£ Stopwords ‚Äì EN explicit (fƒÉrƒÉ overhead)
    remover = StopWordsRemover(
        inputCol="tokens",
        outputCol="filtered_tokens",
        stopWords=StopWordsRemover.loadDefaultStopWords("english"),
        caseSensitive=False
    )

    # 3Ô∏è‚É£ CountVectorizer ‚Äì TF
    cv = CountVectorizer(
        inputCol="filtered_tokens",
        outputCol="tf",
        vocabSize=vocab_size,
        minDF=min_df,
        binary=binary_tf   # üî• HUGE speedup pt clasificare
    )

    # 4Ô∏è‚É£ IDF
    idf = IDF(
        inputCol="tf",
        outputCol=output_col
    )

    pipeline = Pipeline(stages=[tokenizer, remover, cv, idf])

    meta = {
        "input_col": input_col,
        "output_col": output_col,
        "vocab_size": vocab_size,
        "min_df": min_df,
        "binary_tf": binary_tf,
        "stages": [
            "RegexTokenizer",
            "StopWordsRemover",
            "CountVectorizer",
            "IDF"
        ]
    }

    return pipeline, meta
